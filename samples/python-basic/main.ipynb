{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the OpenAI Python package\n",
    "%pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API keys are not used when using the RAPID framework. For the OpenAI SDK, the API key is a necessary parameter for initializing the OpenAI class instance so any string will do. The endpoint URL is where the router service is hosted. In this example, it assumes it's being run locally and accessible on port 8000. Change the URI as/if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "AZURE_OAI_API_KEY = \"NOT_NEEDED_FOR_RAPID_EXAMPLE\"\n",
    "azure_oai_api_version = \"2024-02-01\"\n",
    "azure_oai_endpoint = \"http://localhost:8000/api/v1/dev/apirouter/lb/\"\n",
    "\n",
    "# The appId for the RAPID route that points to the OpenAI deployment\n",
    "rapid_app_id = \"chatbot-gpt4o\"\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=AZURE_OAI_API_KEY,\n",
    "    api_version=azure_oai_api_version,\n",
    "    azure_endpoint = azure_oai_endpoint\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of the Azure OpenAI model deployment name, the app Id of the available routers is used. Azure API Management gateway will route the app to the corresponding defined Azure OpenAI model and also apply any of the enabled features in RAPID (e.g., caching)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model = rapid_app_id,\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Does Azure OpenAI support customer managed keys?\"},\n",
    "        {\"role\": \"assistant\",\n",
    "            \"content\": \"Yes, customer managed keys are supported by Azure OpenAI.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Do other Azure AI services support this too?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Assistant response: {response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using State / Chat History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state management feature must be enabled and configured correctly for this to work. Conversation history is stored in a database and processed by the framework based on the `x-thread-id` header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_id = None\n",
    "\n",
    "# Use the with_raw_response method to get the headers to extract the thread ID\n",
    "response = client.chat.completions.with_raw_response.create(\n",
    "    model = rapid_app_id,\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Does Azure OpenAI support customer managed keys?\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "thread_id = response.headers.get(\"x-thread-id\")\n",
    "completion = response.parse()\n",
    "if thread_id:\n",
    "    print(f\"Thread ID: {thread_id}\\nAssistant response: {completion.choices[0].message.content}\")\n",
    "else:\n",
    "    print(f\"Assistant response: {completion.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a new thread Id generated, you can reference this header value in future request for preserving chat history as part of the current conversation session and reference that history as additional context to the LLM.\n",
    "\n",
    "_Note:_ To start a new session, remove the `x-thread-id` header in your request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_header = {\"x-thread-id\": thread_id}\n",
    "\n",
    "response2 = client.chat.completions.create(\n",
    "    model = rapid_app_id,\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"Do other Azure AI services support this too?\"},\n",
    "    ],\n",
    "    extra_headers = thread_header\n",
    ")\n",
    "\n",
    "print(f\"Assistant response: {response2.choices[0].message.content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
